{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings/info messages\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 12:30:44.011479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-22 12:30:44.011508: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from text_preprocessing import CleanText\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News pieces are grouped within subfolders in ./data directory. Names of subfolder mean category to which news piece is related to. \n",
    "#### Loading data is done using load_data() function from utils module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = load_data('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuations, stop words, digits, whitespaces, empty lines from each news piece were removed. Also words were converted to their base form using lemmatization technique. These actions were done using CleanText class and its methods from text_preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Text_cleaned'] = apply_many_functions(df_data['Text'], lambda text: text.lower(),\n",
    "              lambda text: CleanText(text).remove_punctuations(),\n",
    "              lambda text: CleanText(text).remove_stop_words(), \n",
    "              lambda text: CleanText(text).remove_digits(), \n",
    "              lambda text: CleanText(text).remove_whitespaces(), \n",
    "              lambda text: CleanText(text).remove_empty_lines(),\n",
    "              lambda text: CleanText(text).lemmatize_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In classification I will use pretrained word embeddings. So, I downloaded spaCy library and  its trained word embeddings. I decided to use spaCy because the embeddings load faster than for example the GloVe ones. For this case we can also use word2vec vectors using gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you don't have spacy model already downloaded, pease uncomment the line below.\n",
    "#! python -m spacy download en_core_web_sm \n",
    "spacy_nlp_object = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using count_word_freq() function from utils module I counted how many times each word appeared in the text corpus. Text corpus is all news pieces.\n",
    "\n",
    "#### During tokenization words that appeared in text corpus more than 1 time were took into account.  Tokenizer vectorized a text corpus into a list of integers. and after that text sequences were padded by zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_dict = count_word_freq(list(df_data.Text_cleaned))\n",
    "vocab_size = len([(word, count) for (word, count) in word_freq_dict.items() if count > 1])\n",
    "tokenizer_object = Tokenizer(num_words=vocab_size + 1)\n",
    "text_pad_sequences = text_to_keras_sequence(tokenizer_object, list(df_data.Text_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  I will use trained embeddings in further model, so I need to create embedding matrix for it. Each encoded word  from tokenizer object will be a row index in the embedding matrix and the vector  for that word will be the the vector from the spaCy model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_spacy_embedding_matrix(tokenizer_object, spacy_nlp_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels were converted to categorical data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_labels = transform_dependent_variable(np.array(df_data['Label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying simple neural network architecture also for this case we can try CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(tokenizer_object.num_words, embedding_matrix.shape[1], weights=[embedding_matrix],\n",
    "                  trainable=False)) \n",
    "model.add(LSTM(64,return_sequences=True,dropout=0.1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16,activation='relu')) \n",
    "model.add(Dense(3,activation='softmax')) \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=[\"acc\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uczenie sieci neuronowej without splitting to train/test/val dataset, because we have not enouhg source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 12:36:02.162603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 2s - loss: 1.2593 - acc: 0.3529\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 1.1466 - acc: 0.3529\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 1.0987 - acc: 0.3529\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 1.0681 - acc: 0.3529\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 1.0439 - acc: 0.4706\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 1.0193 - acc: 0.5294\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.9962 - acc: 0.7059\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.9620 - acc: 0.7059\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.9390 - acc: 0.7059\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.9075 - acc: 0.7059\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(text_pad_sequences,categorical_labels,\n",
    "                  epochs=10,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and accuracy for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(text_pad_sequences,categorical_labels, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
